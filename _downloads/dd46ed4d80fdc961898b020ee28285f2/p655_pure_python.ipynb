{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Reducing p655 in Pure Python\n\n\nExperiment p655 was run in 2005 by Mckiernan, Rathbun, and Rowe. It was reduced by\nChris Marone. Here we use the xlook r file parser to run that r file and then get it into the\ndictionary of quantity arrays like we use everywhere else and do some simple plotting\nof the experiment.\n\nFor the curious, this experiment is determining the frictional response of \"Ghost Rocks\"\nfrom Kodiak Alaska.\n\nWhile being able to run r files is great, we'd like to move to reducing the data in Python\nto really get at the full power of pylook. There are lots of ways that pylook can simplify\nreduction and it is easier to follow exactly what is happening for those unfamiliar with the\nr file format.\n\nLet's start out with some imports to get rolling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\nfrom pylook.units import units\nimport pylook.calc as lc\nfrom pylook.cbook import get_test_data\nfrom pylook.io import read_binary\nimport numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_path = get_test_data('p655intact100l')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data, _ = read_binary(data_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The time column in these data files is really the sample rate in Hz. We want to turn that\ninto a delta time and cumulatively sum to get experiment elapsed time. Notice that we are\nassigning units by multiplying them - easy! We have to take the magnitude of time for\n`cumsum` because numpy silently drops our units. We'll fix that with a `cumsum` wrapper\nin pylook soon.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data['Time'] = np.cumsum(1 / data['Time'].m) * units('s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we need to apply calibrations and units to the data - the calibrations are determined\nbased on the sensor used by the experimentalist. We'll get a list of the current data column\nnames so we know what they are called in the look file.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data['Vert_Disp'] = data['Vert_Disp'] * 0.076472 * units('micron / bit')\ndata['Vert_Load'] = data['Vert_Load'] * 1.597778959e-3 * units('MPa / bit')\ndata['Hor_Disp'] = data['Hor_Disp'] * 0.11017176 * units('micron / bit')\ndata['Hor_Load.'] = data['Hor_Load.'] * 3.31712805707e-3 * units('MPa / bit')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Now that calibrations are applied, we really should rename those columns to something more\n# useful. Look limited the length of column names, but pylook does not. We even allow spaces!\n# The `pop` method removes that key/value pair from the dictionary and we reassign it to a\n# new key.\n\ndata['Shear Displacement'] = data.pop('Vert_Disp')\ndata['Shear Stress'] = data.pop('Vert_Load')\ndata['Normal Displacement'] = data.pop('Hor_Disp')\ndata['Normal Stress'] = data.pop('Hor_Load.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There was a spike in the data that the experimentalist decided to remove. We can do that\nwith an offset that sets everything in between the rows to the final value.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data['Shear Stress'] = lc.remove_offset(data['Shear Stress'], 4075, 4089, set_between=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The testing machine expands when loading because it has a finite stiffness. That stiffness\nis known and can be removed from the data to create an elastically corrected displacement.\nPython is great with units here and can help you calculate the stiffness in the right units.\nWe know the stiffness is 0.37 kN/micron and this experiment use samples that were 5x5cm.\nFrom that we let the units library do all of the hard work of calculating the stiffness in\nterms of stress.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "normal_stiffness = 0.37 * units('kN/micron')\nsample_normal_area = 5 * units('cm') * 5 * units('cm')\nnormal_stiffness = sample_normal_area / normal_stiffness\nprint(normal_stiffness.to('micron/MPa'))  # To just displays things in units that we expect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "pylook's elastic correction is a polynomial tool, so you can use as many coefficients as\nyou'd like. Just like `np.polyval` we expect coefficients from the highest order to lowest.\nUnits matter!\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data['Normal Displacement'] = lc.elastic_correction(data['Normal Stress'], data['Normal Displacement'],\n                                                    [normal_stiffness, 0 * units('micron')])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start recording data before there is any load on the samples, so we need to find where\nthe load is brought on. Easiest way to do this is to make a quick plot and find the row\nnumber at which we want to zero things. The `.m` after the data is a way to drop units\n(the magnitude) and is required as currently bokeh does not play well with units.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from bokeh.plotting import figure, show\nfrom bokeh.io import output_notebook\nfrom bokeh.layouts import gridplot, row, column\n\noutput_notebook()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "p = figure(title='Find the normal stress zero row', tools='box_zoom, reset, hover')\np.line(data['rec_num'].m, data['Normal Stress'].m)\nshow(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Row 42 looks pretty good, so we zero the normal stress there. We'd also like to set\neverything before that row to zero since it's just noise. In r files that took some math\ncommands, we zero has options in pylook! We also don't need to worry about adding small\nvalues to avoid divide by zero errors as the friction calculation handles that properly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data['Normal Stress'] = lc.zero(data['Normal Stress'], 42, mode='before')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While we are zeroing, it is a good time to deal with the normal displacement as well.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data['Normal Displacement'] = lc.remove_offset(data['Normal Displacement'], 0, 42,\n                                               set_between=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we need to find the zero point for the shear load and stress.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "p = figure(title='Find the shear stress zero row', tools='box_zoom, reset, hover')\np.line(data['rec_num'].m, data['Shear Stress'].m)\nshow(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Row 1518 looks like a reasonable choice - so lets' zero those two and zero everything\nbefore then.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data['Shear Displacement'] = lc.zero(data['Shear Displacement'], 1518, mode='before')\ndata['Shear Stress'] = lc.zero(data['Shear Stress'], 1518, mode='before')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The displacement transducer on the shear axis often runs out of travel and has to be reset\nduring the experiment. We need to find those spots and record the rows at which those\noffsets start and stop so we can remove them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "p = figure(title='Find the DCDT offsets', tools='box_zoom, reset, hover')\np.line(data['rec_num'].m, data['Shear Displacement'].m)\nshow(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looks like we had two offsets - rows 18593 to 19058 and rows 66262 to 67830.\nLet's remove those and set the values between to the final value to that data look nice.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data['Shear Displacement'] = lc.remove_offset(data['Shear Displacement'], 18593, 19058, set_between=True)\ndata['Shear Displacement'] = lc.remove_offset(data['Shear Displacement'], 66262, 67830, set_between=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the normal displacement we assume that half of it is in each of the two layers of a\ndouble direct setup. We are going to change the sign such that compaction shows a thinner\nlayer and dialation shows a thicker layer. On the bench the blocks were 89.2 mm thick with\na 4 mm gouge layer. Notice that because of the units awareness we can just add 4 mm and the\nmath works!\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data['Normal Displacement'] = data['Normal Displacement'] * (-0.5 * units('dimensionless'))\ndata['Normal Displacement'] = lc.zero(data['Normal Displacement'], 42, mode='before')\ndata['Normal Displacement'] = data['Normal Displacement'] + 4 * units('mm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's calculate simple friction (shear stress / normal stress). We have a function in pylook\nthat does this intelligently handling divide by zeros when there is no shear stress on the\nsample\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data['Friction'] = lc.friction(data['Shear Stress'], data['Normal Stress'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Look\nWe'll use Bokeh to take a quick look at the data. Matplotlib is the best choice for your\npublication plots, but the speed and interactivity of Bokeh in the notebook is hard to beat.\nWe'll be adding helpers to pylook to make this process easier in the future as well.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This is a handy function that will be integrated into pylook in a more advanced way soon,\n# but demonstrates how to make a flexible plotting function instead of copying and pasting a\n# bunch of code over and over again.\n\ndef make_runplot(data, x_var='Time', y_vars=None, tools='pan,wheel_zoom,box_zoom,reset,save,box_select,hover'):\n    plots = []\n    for col_name in list(data):\n        if col_name == x_var:\n            continue\n        if y_vars and (col_name not in y_vars):\n            continue\n        \n        # First plot is simple, the rest we share the x range with the first\n        if plots == []:\n            p = figure(title=col_name, tools=tools)\n        else:\n            p = figure(title=col_name, tools=tools, x_range=plots[0].x_range)\n        \n        # Plot the data and set the labels\n        p.xaxis.axis_label = str(data[x_var].units)\n        p.yaxis.axis_label = str(data[col_name].units)\n        p.line(data[x_var].m, data[col_name].m)\n        \n        plots.append(p)\n    show(gridplot(plots, ncols=1, plot_width=600, plot_height=175))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default make_runplot would plot all of the variables, let's just plot a couple of basic\nones. Hover over the graph to see the values! That can be turned off by clicking the message\nbubble icon in the plot toolbar. If we don't specify, data are plotted with respect to time.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "make_runplot(data, y_vars=['Shear Stress', 'Normal Stress'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can specify to plot relative to another x variable though - with load point displacement\nprobably being the most common.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "make_runplot(data, x_var='Shear Displacement', y_vars=['Shear Stress', 'Normal Stress'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Easy right? We'll be adding more features to reduce this code even further, but it moves us\neven further towards the goal of easy to understand and portable reductions that require a\nminimum of installation and learning pain. Being in Python also means we can seamlessly\ntransfer these data into machine learning tools or just about any other analysis library\nyou're interested in!\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}